<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.3" rel="stylesheet" type="text/css" />



  <link rel="icon" type="image/png" sizes="32x32" href="/favicon.ico?v=5.1.3">







  <meta name="keywords" content="Python," />










<meta name="description" content="Python3 Scrapy 框架的简单使用">
<meta name="keywords" content="Python">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy">
<meta property="og:url" content="http://www.lxhsec.com/2017/11/28/scrapy/index.html">
<meta property="og:site_name" content="lyxhh">
<meta property="og:description" content="Python3 Scrapy 框架的简单使用">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/scrapy_all.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/startproject.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/item1.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/freebuftest.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/spider.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/%E5%88%86%E6%9E%901.gif">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/%E5%88%86%E6%9E%902.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/nouseitem.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/spider1.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/itempipeline.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/itempipeline2.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/extract.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/scrapygenjin.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/xiaoguotu.gif">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/free.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/post2.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/post1.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/post3.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/tools1.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/mongo.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/mongogui.png">
<meta property="og:image" content="http://owrmua5nw.bkt.clouddn.com/mysql1.png">
<meta property="og:updated_time" content="2017-12-01T09:45:13.478Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Scrapy">
<meta name="twitter:description" content="Python3 Scrapy 框架的简单使用">
<meta name="twitter:image" content="http://owrmua5nw.bkt.clouddn.com/scrapy_all.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.3',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://www.lxhsec.com/2017/11/28/scrapy/"/>





  <title>Scrapy | lyxhh</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">lyxhh</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.lxhsec.com/2017/11/28/scrapy/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="lyxhh">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/1.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="lyxhh">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Scrapy</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-11-28T19:15:13+08:00">
                2017-11-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Scrapy/" itemprop="url" rel="index">
                    <span itemprop="name">Scrapy</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Python3 Scrapy 框架的简单使用<a id="more"></a><br><a href="http://doc.scrapy.org/en/latest" target="_blank" rel="external">Scrapy框架官方网址</a></p>
<p><a href="http://scrapy-chs.readthedocs.io/zh_CN/latest/index.html" target="_blank" rel="external">Scrapy中文维护站点</a></p>
<h1 id="Scrapy"><a href="#Scrapy" class="headerlink" title="Scrapy"></a>Scrapy</h1><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。 </div><div class="line"></div><div class="line">可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序中。</div></pre></td></tr></table></figure>
<h2 id="基本结构"><a href="#基本结构" class="headerlink" title="基本结构"></a>基本结构</h2><ul>
<li>首先我们先来看一张图<br><img src="http://owrmua5nw.bkt.clouddn.com/scrapy_all.png" alt="scrapy_all"></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">scrapy的工作流程图</div><div class="line"></div><div class="line">Scrapy engine 引擎是整个框架的核心，由scrapy实现好的。用来发送指令。</div><div class="line"></div><div class="line">Scheduler 是调度器 专门用来发送请求的。</div><div class="line"></div><div class="line">Downloader 下载器 专门用来下载数据的</div><div class="line"></div><div class="line">Item Pipeline 管道文件 专门用来保存数据的</div><div class="line"></div><div class="line">Spiders  我们自己写的爬虫</div><div class="line"></div><div class="line">而我们只需要写Spiders 以及 Item Pipeline</div></pre></td></tr></table></figure>
<h2 id="基本流程"><a href="#基本流程" class="headerlink" title="基本流程"></a>基本流程</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">首先从我们写的爬虫开始 即Spiders, 一开始没有数据，</div><div class="line">因此先发送请求，请求交给引擎，引擎判断是请求，直接交给Scheduler，引擎不做处理</div><div class="line"></div><div class="line">Scheduler接受到请求之后，把请求交给Downloader，让Downloader去网上下载，Scheduler不做处理</div><div class="line"></div><div class="line">Downloader把下载完的数据(即html源码)交回给Spiders</div><div class="line"></div><div class="line">Spiders接受到Downloader下载完的数据后判断，</div><div class="line">如果是请求，直接通过引擎交给Scheduler，继续爬去</div><div class="line"></div><div class="line">如果是数据，直接交给Item Pipeline，做数据处理，</div></pre></td></tr></table></figure>
<h2 id="入门教程"><a href="#入门教程" class="headerlink" title="入门教程"></a>入门教程</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">本篇教程中将带您完成下列任务:</div><div class="line">1.创建一个Scrapy项目</div><div class="line">2.定义提取的Item</div><div class="line">3.编写爬取网站的 spider 并提取 Item</div><div class="line">4.编写 Item Pipeline 来存储提取到的Item(即数据)</div></pre></td></tr></table></figure>
<h3 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h3><p>scrapy startproject freebuf</p>
<p><img src="http://owrmua5nw.bkt.clouddn.com/startproject.png" alt="startproject"></p>
<h3 id="定义item"><a href="#定义item" class="headerlink" title="定义item"></a>定义item</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">Item 是保存爬取到的数据的容器；</div><div class="line">其使用方法和python字典类似</div><div class="line">我们根据自己的需求，建立不同的字段，</div><div class="line">这里以爬去freebuf的url，title 为例。</div><div class="line">因此建立相应的字段。</div></pre></td></tr></table></figure>
<p><img src="http://owrmua5nw.bkt.clouddn.com/item1.png" alt="item1"></p>
<h3 id="编写第一个爬虫"><a href="#编写第一个爬虫" class="headerlink" title="编写第一个爬虫"></a>编写第一个爬虫</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">Spider是用户编写用于从单个网站(或者一些网站)爬取数据的类。</div><div class="line">scrapy.Spider 基础爬虫类。</div><div class="line"></div><div class="line">name: 用于区别Spider。 因此具有唯一性</div><div class="line"></div><div class="line">start_urls: 包含了Spider在启动时进行爬取的url列表。</div><div class="line"></div><div class="line">parse() 是spider的一个方法。 </div><div class="line"></div><div class="line">被调用时，每个初始URL完成下载后生成的 Response 对象将会作为唯一的参数传递给该函数。 </div><div class="line"></div><div class="line">该方法负责解析返回的数据(response data)，提取数据(生成item)以及生成需要进一步处理的URL的 Request 对象。</div><div class="line"></div><div class="line"></div><div class="line">这里分析freebuf</div><div class="line"></div><div class="line">发现 url变化如下</div><div class="line">http://www.freebuf.com/vuls/page/1  第一页的文章地址</div><div class="line">http://www.freebuf.com/vuls/page/2  第二页的文章地址</div><div class="line">http://www.freebuf.com/vuls/page/3  第三页的文章地址</div></pre></td></tr></table></figure>
<p><img src="http://owrmua5nw.bkt.clouddn.com/freebuftest.png" alt="freebuftest"></p>
<p><img src="http://owrmua5nw.bkt.clouddn.com/spider.png" alt="spider"></p>
<h3 id="分析"><a href="#分析" class="headerlink" title="分析"></a>分析</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">我们打开谷歌开发者工具，选取我们想要爬去的字段。</div><div class="line"></div><div class="line">我们可以看到freebuf文章都是相同格式，</div><div class="line">因此寻找一下它们是否有相同的根节点，然后从根节点开始匹配数据。</div><div class="line">这里用使用xpath匹配。</div><div class="line"></div><div class="line">谷歌插件 XPath helper</div><div class="line"></div><div class="line">. 表示当前节点</div><div class="line">// 表示任意位置</div><div class="line">.. 表示当前节点的父节点  上一级的意思。</div><div class="line">@ 选取属性</div><div class="line"></div><div class="line">这里就不说xpath怎么使用了。。直接去看官方文档，</div></pre></td></tr></table></figure>
<p><a href="http://www.w3school.com.cn/xpath/" target="_blank" rel="external">xpath教程</a></p>
<p><img src="http://owrmua5nw.bkt.clouddn.com/%E5%88%86%E6%9E%901.gif" alt="分析1"></p>
<p><img src="http://owrmua5nw.bkt.clouddn.com/%E5%88%86%E6%9E%902.png" alt="分析2"></p>
<h3 id="爬取"><a href="#爬取" class="headerlink" title="爬取"></a>爬取</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"># if self.offset &lt;= 2:</div><div class="line">   	# self.offset += 1</div><div class="line">        # yield scrapy.Request(url=self.url + str(self.offset), callback = self.parse)</div><div class="line"># 发送新的url GET请求加入待爬队列，并调用回调函数 self.parse</div><div class="line"></div><div class="line"># 可以使用 yield scrapy.FormRequest(url, formdata, callback)方法发送POST请求。</div><div class="line"></div><div class="line">scrapy crawl &quot;自己写的爬虫的name&quot; -o 1.json </div><div class="line"></div><div class="line">-o 指定存储的路径</div><div class="line"></div><div class="line">不使用Item Pipeline，存取数据如下。</div></pre></td></tr></table></figure>
<p><img src="http://owrmua5nw.bkt.clouddn.com/nouseitem.png" alt="nouseitem"></p>
<p><img src="http://owrmua5nw.bkt.clouddn.com/spider1.png" alt="spider1"></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">scrapy crawl &quot;自己写的爬虫的name&quot; </div><div class="line"></div><div class="line">使用Item Pipeline，存取数据如下。</div></pre></td></tr></table></figure>
<p><img src="http://owrmua5nw.bkt.clouddn.com/itempipeline.png" alt="itempipeline"></p>
<p><img src="http://owrmua5nw.bkt.clouddn.com/itempipeline2.png" alt="itempipeline2"></p>
<h3 id="extract"><a href="#extract" class="headerlink" title="extract()"></a>extract()</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">Scrapy Shell</div><div class="line"></div><div class="line">Scrapy终端是一个交互终端，我们可以在未启动spider的情况下尝试及调试代码，</div><div class="line">也可以用来测试XPath或CSS表达式，查看他们的工作方式，</div><div class="line">方便我们爬取的网页中提取的数据。</div><div class="line"></div><div class="line"></div><div class="line">scrapy shell &quot;http://www.freebuf.com/vuls/page/1&quot;</div><div class="line"></div><div class="line">extract(): 序列化该节点为unicode字符串并返回list。</div></pre></td></tr></table></figure>
<p><img src="http://owrmua5nw.bkt.clouddn.com/extract.png" alt="extract"></p>
<h2 id="CrawlSpiders"><a href="#CrawlSpiders" class="headerlink" title="CrawlSpiders"></a>CrawlSpiders</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">本篇教程中将带您完成下列任务:</div><div class="line">1.CrawlSpiders的使用</div><div class="line">2.DOWNLOADER_MIDDLEWARES 下载中间件的使用</div><div class="line">3.setting文件参数。</div><div class="line">4.如何抓取图片</div><div class="line">5.如果是想抓取链接里面的内容，则可以把抓取到的链接，</div><div class="line">当作请求在次发送出去，然后给定一个callback，在callback的函数里，提取链接里你想要的内容。</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">通过下面的命令可以快速创建 CrawlSpider模板 的代码：</div><div class="line"></div><div class="line">scrapy genspider -t crawl tencent tencent.com</div><div class="line"></div><div class="line">Spider的派生类</div><div class="line"></div><div class="line">CrawlSpider类定义了一些规则(rule)提供 跟进link的方便 的机制，从爬取的网页中 获取link 并继续爬取的工作更适合。</div></pre></td></tr></table></figure>
<h3 id="settings-py"><a href="#settings-py" class="headerlink" title="settings.py"></a>settings.py</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div></pre></td><td class="code"><pre><div class="line"># -*- coding: utf-8 -*-</div><div class="line"></div><div class="line"></div><div class="line">BOT_NAME = &apos;freebuf&apos;</div><div class="line"></div><div class="line">SPIDER_MODULES = [&apos;freebuf.spiders&apos;]</div><div class="line">NEWSPIDER_MODULE = &apos;freebuf.spiders&apos;</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">USER_AGENT = &apos;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36&apos;</div><div class="line"></div><div class="line">USER_AGENTS = [</div><div class="line">    &apos;Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.0)&apos;,</div><div class="line">    &apos;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.2)&apos;,</div><div class="line">    &apos;Opera/9.27 (Windows NT 5.2; U; zh-cn)&apos;,</div><div class="line">    &apos;Opera/8.0 (Macintosh; PPC Mac OS X; U; en)&apos;,</div><div class="line">    &apos;Mozilla/5.0 (Macintosh; PPC Mac OS X; U; en) Opera 8.0&apos;,</div><div class="line">    &apos;Mozilla/5.0 (Linux; U; Android 4.0.3; zh-cn; M032 Build/IML74K) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30&apos;,</div><div class="line">    &apos;Mozilla/5.0 (Windows; U; Windows NT 5.2) AppleWebKit/525.13 (KHTML, like Gecko) Chrome/0.2.149.27 Safari/525.13&apos;</div><div class="line">]</div><div class="line"></div><div class="line">PROXIES = [</div><div class="line">        &#123;&quot;ip_prot&quot; :&quot;61.135.217.7:80&quot;, &quot;user_passwd&quot; : &quot;&quot;&#125;,</div><div class="line">        &#123;&quot;ip_prot&quot; :&quot;221.10.159.234:1337&quot;, &quot;user_passwd&quot; : &quot;&quot;&#125;,</div><div class="line">        &#123;&quot;ip_prot&quot; :&quot;220.249.185.178:9999&quot;, &quot;user_passwd&quot; : &quot;&quot;&#125;,</div><div class="line">        &#123;&quot;ip_prot&quot; :&quot;116.23.137.56:9999&quot;, &quot;user_passwd&quot; : &quot;&quot;&#125;,</div><div class="line">        &#123;&quot;ip_prot&quot; :&quot;183.56.177.130:808&quot;, &quot;user_passwd&quot; : &quot;&quot;&#125;,</div><div class="line">        &#123;&quot;ip_prot&quot; :&quot;221.214.214.144:53281&quot;, &quot;user_passwd&quot; : &quot;&quot;&#125;,</div><div class="line">        &#123;&quot;ip_prot&quot; :&quot;61.155.164.109:3128&quot;, &quot;user_passwd&quot; : &quot;&quot;&#125;,</div><div class="line">        &#123;&quot;ip_prot&quot; :&quot;218.56.132.158:8080&quot;, &quot;user_passwd&quot; : &quot;&quot;&#125;,</div><div class="line">        &#123;&quot;ip_prot&quot; :&quot;113.200.159.155:9999&quot;, &quot;user_passwd&quot; : &quot;&quot;&#125;,</div><div class="line">        &#123;&quot;ip_prot&quot; :&quot;123.7.38.31:9999&quot;, &quot;user_passwd&quot; : &quot;&quot;&#125;,</div><div class="line">        &#123;&quot;ip_prot&quot; :&quot;61.163.39.70:9999&quot;, &quot;user_passwd&quot; : &quot;&quot;&#125;</div><div class="line">]</div><div class="line"># Obey robots.txt rules  是否遵从robots.txt 协议</div><div class="line"># ROBOTSTXT_OBEY = True</div><div class="line"></div><div class="line"># Configure maximum concurrent requests performed by Scrapy (default: 16)   同时可以处理多少个请求。默认16</div><div class="line">#CONCURRENT_REQUESTS = 32</div><div class="line"></div><div class="line"># Configure a delay for requests for the same website (default: 0)</div><div class="line"></div><div class="line"></div><div class="line"># 下载延迟，以秒为单位</div><div class="line">DOWNLOAD_DELAY = random.random()+0.5</div><div class="line"># 默认情况下，Scrapy在两个请求间不等待一个固定的值， 而是# 使用0.5到1.5之间的一个随机值 </div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line"># The download delay setting will honor only one of:</div><div class="line">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</div><div class="line">#CONCURRENT_REQUESTS_PER_IP = 16</div><div class="line"></div><div class="line"># Disable cookies (enabled by default)</div><div class="line">COOKIES_ENABLED = False  #  禁用cookie，有些网站通过cookie来判断是否有爬虫</div><div class="line"></div><div class="line">DOWNLOAD_TIMEOUT = 15  # 减小下载超时（默认: 180）</div><div class="line"></div><div class="line">RETRY_ENABLED = False    # 禁止重试</div><div class="line">REDIRECT_ENABLED = False   # 禁止重定向</div><div class="line"># Disable Telnet Console (enabled by default)</div><div class="line">#TELNETCONSOLE_ENABLED = False</div><div class="line"></div><div class="line"># Override the default request headers:</div><div class="line">#DEFAULT_REQUEST_HEADERS = &#123;</div><div class="line">#   &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&apos;,</div><div class="line">#   &apos;Accept-Language&apos;: &apos;en&apos;,</div><div class="line">#&#125;</div><div class="line"></div><div class="line"># Enable or disable spider middlewares</div><div class="line"># See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html</div><div class="line">#SPIDER_MIDDLEWARES = &#123;</div><div class="line">#    &apos;freebuf.middlewares.FreebufSpiderMiddleware&apos;: 543,</div><div class="line">#&#125;</div><div class="line"></div><div class="line"># Enable or disable downloader middlewares</div><div class="line"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</div><div class="line"></div><div class="line"># 下载中间件是处于引擎和下载器之间的一层组件，可以有多个下载中间件被加载运行。</div><div class="line"></div><div class="line"># 当引擎传递请求给下载器的过程中，下载中间件可以对请求进行处理 （例如增加http header信息，增加proxy信息等）；</div><div class="line"></div><div class="line"># 在下载器完成http请求，传递响应给引擎的过程中， 下载中间件可以对响应进行处理（例如进行gzip的解压等）</div><div class="line"></div><div class="line">DOWNLOADER_MIDDLEWARES = &#123;</div><div class="line">    &apos;freebuf.middlewares.RandomUserAgent&apos;: 100,</div><div class="line">    &apos;freebuf.middlewares.RandomProxy&apos;: 101,</div><div class="line">   # &apos;freebuf.middlewares.MyCustomDownloaderMiddleware&apos;: 543,</div><div class="line">&#125;</div><div class="line"></div><div class="line"></div><div class="line"># 可选的级别有: CRITICAL、 ERROR、WARNING、INFO、DEBUG 。</div><div class="line"># CRITICAL - 严重错误(critical)</div><div class="line"></div><div class="line"># ERROR - 一般错误(regular errors)</div><div class="line"># WARNING - 警告信息(warning messages)</div><div class="line"># INFO - 一般信息(informational messages)</div><div class="line"># DEBUG - 调试信息(debugging messages)</div><div class="line"># log日志文件，当前目录里创建lagou.log。</div><div class="line"># LOG_FILE = &quot;lagou.log&quot;</div><div class="line"># 记录INFO级别及其以上的日志信息</div><div class="line"># LOG_LEVEL = &quot;INFO&quot;</div><div class="line"></div><div class="line"># Enable or disable extensions</div><div class="line"># See http://scrapy.readthedocs.org/en/latest/topics/extensions.html</div><div class="line">#EXTENSIONS = &#123;</div><div class="line">#    &apos;scrapy.extensions.telnet.TelnetConsole&apos;: None,</div><div class="line">#&#125;</div><div class="line"></div><div class="line"># Configure item pipelines</div><div class="line"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</div><div class="line"># 管道文件，值越小优先级越高</div><div class="line">ITEM_PIPELINES = &#123;</div><div class="line">   &apos;freebuf.pipelines.FreebufPipeline&apos;: 300,</div><div class="line">   &apos;freebuf.pipelines.ImagesPipeline&apos;: 301,</div><div class="line">&#125;</div><div class="line"></div><div class="line"># Enable and configure the AutoThrottle extension (disabled by default)</div><div class="line"># See http://doc.scrapy.org/en/latest/topics/autothrottle.html</div><div class="line">#AUTOTHROTTLE_ENABLED = True</div><div class="line"># The initial download delay</div><div class="line">#AUTOTHROTTLE_START_DELAY = 5</div><div class="line"># The maximum download delay to be set in case of high latencies</div><div class="line">#AUTOTHROTTLE_MAX_DELAY = 60</div><div class="line"># The average number of requests Scrapy should be sending in parallel to</div><div class="line"># each remote server</div><div class="line">#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</div><div class="line"># Enable showing throttling stats for every response received:</div><div class="line">#AUTOTHROTTLE_DEBUG = False</div><div class="line"></div><div class="line"># Enable and configure HTTP caching (disabled by default)</div><div class="line"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</div><div class="line">#HTTPCACHE_ENABLED = True</div><div class="line">#HTTPCACHE_EXPIRATION_SECS = 0</div><div class="line">#HTTPCACHE_DIR = &apos;httpcache&apos;</div><div class="line">#HTTPCACHE_IGNORE_HTTP_CODES = []</div><div class="line">#HTTPCACHE_STORAGE = &apos;scrapy.extensions.httpcache.FilesystemCacheStorage&apos;</div></pre></td></tr></table></figure>
<h3 id="middlewares-py"><a href="#middlewares-py" class="headerlink" title="middlewares.py"></a>middlewares.py</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"># -*- coding: utf-8 -*-</div><div class="line"></div><div class="line">from scrapy import signals</div><div class="line">import random</div><div class="line">from .settings import USER_AGENTS</div><div class="line">from .settings import PROXIES</div><div class="line">import base64</div><div class="line"></div><div class="line"># process_request(self, request, spider)</div><div class="line"># 当每个request通过下载中间件时，该方法被调用。</div><div class="line"></div><div class="line"># 随机的User-Agent</div><div class="line">class RandomUserAgent(object):</div><div class="line">    def process_request(self, request, spider):</div><div class="line"></div><div class="line">        useragent = random.choice(USER_AGENTS)</div><div class="line">        # print(useragent)</div><div class="line">        request.headers.setdefault(&quot;User-Agent&quot;, useragent)</div><div class="line"></div><div class="line">class RandomProxy(object):</div><div class="line">    def process_request(self, request, spider):</div><div class="line">        proxy = random.choice(PROXIES)</div><div class="line"></div><div class="line">        if proxy[&apos;user_passwd&apos;] is None:</div><div class="line">            # 没有代理账户验证的代理使用方式</div><div class="line">            request.meta[&apos;proxy&apos;] = &quot;http://&quot; + proxy[&apos;ip_port&apos;]</div><div class="line"></div><div class="line">        else:</div><div class="line">            </div><div class="line">            # 对账户密码进行base64编码转换</div><div class="line">            base64_userpasswd = base64.b64encode(proxy[&apos;user_passwd&apos;])</div><div class="line">            # 对应到代理服务器的信令格式里</div><div class="line">            request.headers[&apos;Proxy-Authorization&apos;] = &apos;Basic &apos; + base64_userpasswd</div><div class="line"></div><div class="line">            request.meta[&apos;proxy&apos;] = &quot;http://&quot; + proxy[&apos;ip_port&apos;]</div></pre></td></tr></table></figure>
<h3 id="pipelines-py"><a href="#pipelines-py" class="headerlink" title="pipelines.py"></a>pipelines.py</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"># -*- coding: utf-8 -*-</div><div class="line"></div><div class="line"># Define your item pipelines here</div><div class="line">#</div><div class="line"># Don&apos;t forget to add your pipeline to the ITEM_PIPELINES setting</div><div class="line"># See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html</div><div class="line"></div><div class="line">import json</div><div class="line">import os</div><div class="line">import scrapy</div><div class="line">from .settings import IMAGES_STORE</div><div class="line">from scrapy.pipelines.images import ImagesPipeline</div><div class="line"># from scrapy.utils.project import get_project_settings</div><div class="line"></div><div class="line"></div><div class="line">class FreebufPipeline(object):</div><div class="line">    def __init__(self):</div><div class="line">        self.filename = open(&quot;free.json&quot;, &quot;w&quot;,encoding=&apos;utf-8&apos;)</div><div class="line"></div><div class="line">    def process_item(self, item, spider):</div><div class="line">        text = json.dumps(dict(item), ensure_ascii = False) + &quot;,\n&quot;</div><div class="line">        self.filename.write(text)</div><div class="line">        return item</div><div class="line"></div><div class="line">    def close_spider(self, spider):</div><div class="line">        self.filename.close()</div><div class="line"></div><div class="line"></div><div class="line">class ImagesPipeline(ImagesPipeline):</div><div class="line"></div><div class="line">    # IMAGES_STORE = get_project_settings().get(&quot;IMAGES_STORE&quot;)</div><div class="line">    IMAGES_STORE = IMAGES_STORE</div><div class="line">    def get_media_requests(self, item, info):</div><div class="line">        image_url = item[&quot;img&quot;]</div><div class="line">        yield scrapy.Request(image_url)</div><div class="line"></div><div class="line">    def item_completed(self, results, item, info):</div><div class="line">        # 固定写法，获取图片路径，同时判断这个路径是否正确，如果正确，就放到 image_path里，ImagesPipeline源码剖析可见</div><div class="line">        image_path = [x[&quot;path&quot;] for ok, x in results if ok]</div><div class="line">        if not image_paths:</div><div class="line">            raise DropItem(&quot;Item contains no images&quot;)</div><div class="line">        os.rename(self.IMAGES_STORE + &quot;/&quot; + image_path[0], self.IMAGES_STORE + &quot;/&quot; + item[&quot;title&quot;] + &quot;.jpg&quot;)</div><div class="line">        item[&quot;img_path&quot;] = self.IMAGES_STORE + &quot;/&quot; + item[&quot;name&quot;]</div><div class="line"></div><div class="line">        return item</div><div class="line"></div><div class="line"># get_media_requests的作用就是为每一个图片链接生成一个Request对象，</div><div class="line"># 这个方法的输出将作为item_completed的输入中的results，</div><div class="line"># results是一个元组，每个元组包括(success, imageinfoorfailure)。</div><div class="line"># 如果success=true，imageinfoor_failure是一个字典，包括url/path/checksum三个key。</div></pre></td></tr></table></figure>
<h3 id="freebufcrawl-py"><a href="#freebufcrawl-py" class="headerlink" title="freebufcrawl.py"></a>freebufcrawl.py</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"># -*- coding: utf-8 -*-</div><div class="line">import scrapy</div><div class="line">from scrapy.linkextractors import LinkExtractor</div><div class="line">from scrapy.spiders import CrawlSpider, Rule</div><div class="line">from freebuf.items import FreebufItem</div><div class="line"></div><div class="line">class FreebufcrawlSpider(CrawlSpider):</div><div class="line">    name = &apos;freebufcrawl&apos;</div><div class="line">    allowed_domains = [&apos;freebuf.com&apos;]</div><div class="line">    start_urls = [&apos;http://www.freebuf.com/vuls/page/1&apos;]</div><div class="line">    </div><div class="line">	# allow=r&apos;/page/\d+&apos; 用正则写</div><div class="line">	# LinkExtractor(allow=r&apos;/page/\d+&apos;)↓</div><div class="line">	# 首先发送start_urls请求，然后根据Response的html源码进行链接的提取，返回的符合匹配规则的链接匹配对象的列表</div><div class="line">	</div><div class="line">	Rule(LinkExtractor(allow=r&apos;/page/\d+&apos;) ↓</div><div class="line">	# 获取这个列表里的链接，依次发送请求，并且继续跟进，调用指定回调函数处理</div><div class="line"></div><div class="line">	# follow=True </div><div class="line">	#指定了根据该规则从response提取的链接是否需要跟进。 </div><div class="line"></div><div class="line">	#如果callback为None，follow 默认设置为True ，否则默认为False。</div><div class="line"></div><div class="line">	# 什么是跟进，就是提取出来的链接，是否重新发送出去。</div><div class="line"></div><div class="line">    rules = (</div><div class="line">        Rule(LinkExtractor(allow=r&apos;/page/\d+&apos;), callback=&apos;parse1_item&apos;, follow=True),</div><div class="line">    )</div><div class="line"></div><div class="line">    def parse1_item(self, response):</div><div class="line">        print(response.url)</div><div class="line">        for each in response.xpath(&apos;//div[@class=&quot;news_inner news-list&quot;]&apos;):</div><div class="line">            i = FreebufItem()</div><div class="line">            i[&apos;title&apos;] = each.xpath(&apos;.//div[@class=&quot;news-info&quot;]/dl/dt/a/text()&apos;).extract()[0]</div><div class="line">            i[&apos;url&apos;] = each.xpath(&apos;.//div[@class=&quot;news-info&quot;]/dl/dt/a/@href&apos;).extract()[0]</div><div class="line">        </div><div class="line">            yield i</div></pre></td></tr></table></figure>
<p><img src="http://owrmua5nw.bkt.clouddn.com/scrapygenjin.png" alt="scrapygenjin"></p>
<h3 id="item-py"><a href="#item-py" class="headerlink" title="item.py"></a>item.py</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">import scrapy</div><div class="line"></div><div class="line"></div><div class="line">class FreebufItem(scrapy.Item):</div><div class="line">    # define the fields for your item here like:</div><div class="line">    # name = scrapy.Field()</div><div class="line">    # pass</div><div class="line">    title = scrapy.Field()</div><div class="line">    url = scrapy.Field()</div><div class="line">    img = scrapy.Field()</div><div class="line">    img_path = scrapy.Field()</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">注意一点：</div><div class="line"> Rule(page_lx, callback = &apos;parse&apos;, follow = True)</div><div class="line"># 回调函数不要写parse </div><div class="line">由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 parse方法，crawl spider将会运行失败。</div></pre></td></tr></table></figure>
<p>full 是用来区分图片和缩略图（如果使用的话）的一个子文件夹。详情参见 <a href="http://docs.pythontab.com/scrapy/scrapy0.24/topics/images.html" target="_blank" rel="external">缩略图生成</a>.</p>
<p><img src="http://owrmua5nw.bkt.clouddn.com/xiaoguotu.gif" alt="xiaoguotu"><br><img src="http://owrmua5nw.bkt.clouddn.com/free.png" alt="free"></p>
<h2 id="模拟POST"><a href="#模拟POST" class="headerlink" title="模拟POST"></a>模拟POST</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">本篇教程中将带您完成下列任务:</div><div class="line">1.完成POST登陆。</div></pre></td></tr></table></figure>
<h3 id="POST请求"><a href="#POST请求" class="headerlink" title="POST请求"></a>POST请求</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">可以使用 yield scrapy.FormRequest(url, formdata, callback)方法发送POST请求。</div><div class="line"></div><div class="line">如果希望程序执行一开始就发送POST请求，可以重写Spider类的start_requests(self) 方法，</div><div class="line">并且不再调用start_urls里的url。</div><div class="line"></div><div class="line">ex.↓。。其他文件没做任何操作。仅仅测试了下登陆。</div></pre></td></tr></table></figure>
<p><img src="http://owrmua5nw.bkt.clouddn.com/post2.png" alt="post2"></p>
<h3 id="模拟用户登录"><a href="#模拟用户登录" class="headerlink" title="模拟用户登录"></a>模拟用户登录</h3><p>使用FormRequest.from_response()方法<a href="http://docs.pythontab.com/scrapy/scrapy0.24/topics/request-response.html#topics-request-response-ref-request-userlogin" target="_blank" rel="external">模拟用户登录</a><br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">通常网站通过 实现对某些表单字段（如数据或是登录界面中的认证令牌等）的预填充。</div><div class="line"></div><div class="line">使用Scrapy抓取网页时，如果想要预填充或重写像用户名、用户密码这些表单字段， </div><div class="line">可以使用 FormRequest.from_response() 方法实现。</div><div class="line"></div><div class="line">ex.↓。。其他文件没做任何操作。仅仅测试了下登陆。</div></pre></td></tr></table></figure></p>
<p><img src="http://owrmua5nw.bkt.clouddn.com/post1.png" alt="post1"></p>
<h3 id="Cookie"><a href="#Cookie" class="headerlink" title="Cookie"></a>Cookie</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">实在没办法，直接使用保存登陆状态的Cookie模拟登陆</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"># -*- coding: utf-8 -*-</div><div class="line">import scrapy</div><div class="line"></div><div class="line"></div><div class="line">class ExampleSpider(scrapy.Spider):</div><div class="line">	name = &apos;toolscrawl&apos;</div><div class="line">	allowed_domains = [&apos;t00ls.net&apos;]</div><div class="line">	start_urls = [&apos;https://www.t00ls.net/redirect-42663.html#lastpost&apos;]</div><div class="line"></div><div class="line">	cookies = &#123;</div><div class="line">	&quot;discuz_fastpostrefresh&quot; : &quot;0&quot;,</div><div class="line">	&quot;smile&quot;:&quot;6D1&quot;,</div><div class="line">	&quot;td_cookie&quot;:&quot;18412346722144123069412388952927&quot;,</div><div class="line">	&quot;UTH_cookietime&quot;:&quot;2592000&quot;,</div><div class="line">	&quot;UTH_auth&quot;:&quot;d9d711323ZIHT32212G123123rSpA3T3123fr6Iix4UiQsMJm4n4ZMSCfJVV7PPVavX%2B7Ed9nzhPJDgEPpAUCvrLEKzqeEou5103d8eT0bma6Rr4dMjHOTuQQ&quot;,</div><div class="line">	&quot;UTH_sid&quot;:&quot;MmDXUm&quot;</div><div class="line">	&#125;</div><div class="line"></div><div class="line">	# 可以重写Spider类的start_requests方法，附带Cookie值，发送POST请求</div><div class="line">	def start_requests(self):</div><div class="line">		for url in self.start_urls:</div><div class="line">			yield scrapy.FormRequest(url, cookies = self.cookies, callback = self.parse_page)</div><div class="line"></div><div class="line">	# 处理响应内容</div><div class="line">	def parse_page(self, response):</div><div class="line">		print (&quot;===========&quot; + response.url)</div><div class="line">		with open(&quot;deng.html&quot;, &quot;w&quot;) as filename:</div><div class="line">			filename.write(response.body.decode(&apos;utf-8&apos;))</div></pre></td></tr></table></figure>
<p><img src="http://owrmua5nw.bkt.clouddn.com/post3.png" alt="post3"></p>
<p><img src="http://owrmua5nw.bkt.clouddn.com/tools1.png" alt="tools1"></p>
<h2 id="导入数据库"><a href="#导入数据库" class="headerlink" title="导入数据库"></a>导入数据库</h2><h3 id="Mongodb"><a href="#Mongodb" class="headerlink" title="Mongodb"></a>Mongodb</h3><ul>
<li><p>items.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">import scrapy</div><div class="line"></div><div class="line"></div><div class="line">class DoubanItem(scrapy.Item):</div><div class="line">    # define the fields for your item here like:</div><div class="line">    name = scrapy.Field()</div><div class="line">    pingjia = scrapy.Field()</div><div class="line">    xing = scrapy.Field()</div><div class="line">    jianjie = scrapy.Field()</div><div class="line">    url = scrapy.Field()</div><div class="line">    # images = scrapy.Field()</div></pre></td></tr></table></figure>
</li>
<li><p>setting.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div></pre></td><td class="code"><pre><div class="line"># -*- coding: utf-8 -*-</div><div class="line"></div><div class="line"># Scrapy settings for douban project</div><div class="line">#</div><div class="line"># For simplicity, this file contains only settings considered important or</div><div class="line"># commonly used. You can find more settings consulting the documentation:</div><div class="line">#</div><div class="line">#     http://doc.scrapy.org/en/latest/topics/settings.html</div><div class="line">#     http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</div><div class="line">#     http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html</div><div class="line"></div><div class="line">BOT_NAME = &apos;douban&apos;</div><div class="line"></div><div class="line">SPIDER_MODULES = [&apos;douban.spiders&apos;]</div><div class="line">NEWSPIDER_MODULE = &apos;douban.spiders&apos;</div><div class="line"></div><div class="line">MYSQL_HOST = &apos;127.0.0.1&apos;</div><div class="line"></div><div class="line">MYSQL_PORT = 3306</div><div class="line"></div><div class="line">MYSQL_DBNAME = &apos;douban&apos;</div><div class="line"></div><div class="line">MYSQL_USER = &apos;root&apos;</div><div class="line"></div><div class="line">MYSQL_PASSWORD = &apos;root&apos;</div><div class="line"></div><div class="line"></div><div class="line"># MONGODB 主机环回地址127.0.0.1</div><div class="line"></div><div class="line">MONGODB_HOST = &apos;127.0.0.1&apos;</div><div class="line"></div><div class="line"># 端口号，默认是27017</div><div class="line">MONGODB_PORT = 27017</div><div class="line"></div><div class="line"># 设置数据库名称</div><div class="line">MONGODB_DBNAME = &apos;DouBan&apos;</div><div class="line"></div><div class="line"># 存放本次数据的表名称</div><div class="line">MONGODB_DOCNAME = &apos;DouBanMovies&apos;</div><div class="line"></div><div class="line"></div><div class="line"># Crawl responsibly by identifying yourself (and your website) on the user-agent</div><div class="line"># USER_AGENT = &apos;douban (+http://www.yourdomain.com)&apos;</div><div class="line">USER_AGENT = &apos;Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36&apos;</div><div class="line"></div><div class="line"># Obey robots.txt rules</div><div class="line"># ROBOTSTXT_OBEY = True</div><div class="line"></div><div class="line"># Configure maximum concurrent requests performed by Scrapy (default: 16)</div><div class="line">#CONCURRENT_REQUESTS = 32</div><div class="line"></div><div class="line"># Configure a delay for requests for the same website (default: 0)</div><div class="line"># See http://scrapy.readthedocs.org/en/latest/topics/settings.html#download-delay</div><div class="line"># See also autothrottle settings and docs</div><div class="line">#DOWNLOAD_DELAY = 3</div><div class="line"># The download delay setting will honor only one of:</div><div class="line">#CONCURRENT_REQUESTS_PER_DOMAIN = 16</div><div class="line">#CONCURRENT_REQUESTS_PER_IP = 16</div><div class="line"></div><div class="line"># Disable cookies (enabled by default)</div><div class="line">#COOKIES_ENABLED = False</div><div class="line"></div><div class="line"># Disable Telnet Console (enabled by default)</div><div class="line">#TELNETCONSOLE_ENABLED = False</div><div class="line"></div><div class="line"># Override the default request headers:</div><div class="line"># DEFAULT_REQUEST_HEADERS = &#123;</div><div class="line">  # &apos;Accept&apos;: &apos;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&apos;,</div><div class="line">#   &apos;Accept-Language&apos;: &apos;en&apos;,</div><div class="line"># &#125;</div><div class="line"></div><div class="line"># Enable or disable spider middlewares</div><div class="line"># See http://scrapy.readthedocs.org/en/latest/topics/spider-middleware.html</div><div class="line">SPIDER_MIDDLEWARES = &#123;</div><div class="line">   &apos;douban.middlewares.DoubanSpiderMiddleware&apos;: 543,</div><div class="line">&#125;</div><div class="line"></div><div class="line"># Enable or disable downloader middlewares</div><div class="line"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html</div><div class="line">#DOWNLOADER_MIDDLEWARES = &#123;</div><div class="line">#    &apos;douban.middlewares.MyCustomDownloaderMiddleware&apos;: 543,</div><div class="line">#&#125;</div><div class="line"></div><div class="line"># Enable or disable extensions</div><div class="line"># See http://scrapy.readthedocs.org/en/latest/topics/extensions.html</div><div class="line">#EXTENSIONS = &#123;</div><div class="line">#    &apos;scrapy.extensions.telnet.TelnetConsole&apos;: None,</div><div class="line">#&#125;</div><div class="line"></div><div class="line"># Configure item pipelines</div><div class="line"># See http://scrapy.readthedocs.org/en/latest/topics/item-pipeline.html</div><div class="line">ITEM_PIPELINES = &#123;</div><div class="line">   &apos;douban.pipelines.DoubanPipeline&apos;: 300,</div><div class="line">&#125;</div><div class="line"></div><div class="line"># Enable and configure the AutoThrottle extension (disabled by default)</div><div class="line"># See http://doc.scrapy.org/en/latest/topics/autothrottle.html</div><div class="line">#AUTOTHROTTLE_ENABLED = True</div><div class="line"># The initial download delay</div><div class="line">#AUTOTHROTTLE_START_DELAY = 5</div><div class="line"># The maximum download delay to be set in case of high latencies</div><div class="line">#AUTOTHROTTLE_MAX_DELAY = 60</div><div class="line"># The average number of requests Scrapy should be sending in parallel to</div><div class="line"># each remote server</div><div class="line">#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0</div><div class="line"># Enable showing throttling stats for every response received:</div><div class="line">#AUTOTHROTTLE_DEBUG = False</div><div class="line"></div><div class="line"># Enable and configure HTTP caching (disabled by default)</div><div class="line"># See http://scrapy.readthedocs.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings</div><div class="line">#HTTPCACHE_ENABLED = True</div><div class="line">#HTTPCACHE_EXPIRATION_SECS = 0</div><div class="line">#HTTPCACHE_DIR = &apos;httpcache&apos;</div><div class="line">#HTTPCACHE_IGNORE_HTTP_CODES = []</div><div class="line">#HTTPCACHE_STORAGE = &apos;scrapy.extensions.httpcache.FilesystemCacheStorage&apos;</div></pre></td></tr></table></figure>
</li>
<li><p>doubancrawl.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div></pre></td><td class="code"><pre><div class="line"># -*- coding: utf-8 -*-</div><div class="line"></div><div class="line">import scrapy</div><div class="line">from douban.items import DoubanItem</div><div class="line"></div><div class="line">class DoubancrawlSpider(scrapy.Spider):</div><div class="line">    name = &apos;doubancrawl&apos;</div><div class="line">    allowed_domains = [&apos;douban.com&apos;]</div><div class="line">    offset = 0</div><div class="line">    url = &apos;https://movie.douban.com/top250?start=&apos;</div><div class="line">    starturl = url + str(offset)</div><div class="line">    start_urls = [starturl]</div><div class="line"></div><div class="line">    def parse(self, response):</div><div class="line"></div><div class="line">        for each in response.xpath(&apos;//div[@class=&quot;item&quot;]/div[@class=&quot;info&quot;]&apos;):</div><div class="line">            item = DoubanItem()</div><div class="line">            item[&apos;name&apos;] = each.xpath(&apos;.//a/span[1]/text()&apos;).extract()[0]</div><div class="line">            item[&apos;url&apos;] = each.xpath(&apos;.//a/@href&apos;).extract()[0]</div><div class="line">            item[&apos;pingjia&apos;] = each.xpath(&apos;.//div/span[4]/text()&apos;).extract()[0]</div><div class="line">            item[&apos;xing&apos;] = each.xpath(&apos;.//div/span[2]/text()&apos;).extract()[0]</div><div class="line">            jianjie  = each.xpath(&apos;.//p[@class=&quot;quote&quot;]/span/text()&apos;).extract()</div><div class="line">            if len(jianjie) == 0 :</div><div class="line">                item[&apos;jianjie&apos;] = []</div><div class="line">            else:</div><div class="line">                item[&apos;jianjie&apos;] = each.xpath(&apos;.//p[@class=&quot;quote&quot;]/span/text()&apos;).extract()[0]</div><div class="line"></div><div class="line">            yield item</div><div class="line"></div><div class="line">        if self.offset &lt;= 250:</div><div class="line">            self.offset += 25</div><div class="line">        yield scrapy.Request(url=self.url + str(self.offset), callback = self.parse)</div></pre></td></tr></table></figure>
</li>
<li><p>piplelines.py</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"># -*- coding: utf-8 -*-</div><div class="line"></div><div class="line">import codecs</div><div class="line">import json</div><div class="line">import pymongo</div><div class="line">from .settings import MONGODB_HOST</div><div class="line">from .settings import MONGODB_PORT</div><div class="line">from .settings import MONGODB_DBNAME</div><div class="line">from .settings import MONGODB_DOCNAME</div><div class="line"></div><div class="line">class DoubanPipeline(object):</div><div class="line">    def __init__(self):</div><div class="line">        # 获取setting主机名、端口号和数据库名</div><div class="line">        host = MONGODB_HOST</div><div class="line">        port = MONGODB_PORT</div><div class="line">        dbname = MONGODB_DBNAME</div><div class="line"></div><div class="line">        # client = pymongo.MongoClient(&apos;mongodb://lxhsec:root@localhost:27017/dbname&apos;)</div><div class="line"></div><div class="line">        client = pymongo.MongoClient(&apos;mongodb://localhost:27017&apos;)</div><div class="line">        mdb = client[dbname]</div><div class="line"></div><div class="line">        self.post = mdb[MONGODB_DOCNAME]</div><div class="line"></div><div class="line">    def process_item(self, item, spider):</div><div class="line">        data = dict(item)</div><div class="line">        self.post.insert(data)</div><div class="line">        return item</div></pre></td></tr></table></figure>
</li>
</ul>
<p><a href="http://www.lxhsec.com/2017/10/31/moong1/">MongoDB的使用</a></p>
<p><img src="http://owrmua5nw.bkt.clouddn.com/mongo.png" alt="mongo"><br>查看全部数据可用mongodb gui管理软件robomongo查看</p>
<p><img src="http://owrmua5nw.bkt.clouddn.com/mongogui.png" alt="mongogui"></p>
<h3 id="Mysql"><a href="#Mysql" class="headerlink" title="Mysql"></a>Mysql</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">1.创建数据库</div><div class="line">create database douban charset=&apos;utf8&apos;;</div><div class="line"></div><div class="line">2.创建表</div><div class="line">create table doubanmovie( </div><div class="line">id int not null primary key auto_increment,</div><div class="line">name varchar(20)  unique not null, </div><div class="line">pingjia varchar(10), </div><div class="line">xing varchar(10), </div><div class="line">jianjie varchar(50), </div><div class="line">url varchar(50) );</div></pre></td></tr></table></figure>
<ul>
<li>piplelines.py<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div></pre></td><td class="code"><pre><div class="line"># -*- coding: utf-8 -*-</div><div class="line"></div><div class="line">import pymysql</div><div class="line">from .settings import MYSQL_HOST</div><div class="line">from .settings import MYSQL_PORT</div><div class="line">from .settings import MYSQL_DBNAME</div><div class="line">from .settings import MYSQL_USER</div><div class="line">from .settings import MYSQL_PASSWORD</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">class DoubanPipeline(object):</div><div class="line">    def __init__(self):</div><div class="line">        # 获取setting主机名、端口号和数据库名</div><div class="line"></div><div class="line">        self.conn = pymysql.connect(host = MYSQL_HOST,</div><div class="line">                       port = MYSQL_PORT,</div><div class="line">                       user = MYSQL_USER,</div><div class="line">                       password = MYSQL_PASSWORD,</div><div class="line">                       database = MYSQL_DBNAME,</div><div class="line">                       charset = &apos;utf8&apos;)</div><div class="line">        self.cs = self.conn.cursor()</div><div class="line"></div><div class="line"></div><div class="line">    def process_item(self, item, spider):</div><div class="line">        params = [item[&apos;name&apos;],item[&apos;pingjia&apos;],item[&apos;xing&apos;],item[&apos;jianjie&apos;],item[&apos;url&apos;]]</div><div class="line">        self.cs.execute(&quot;insert into doubanmovie(name,pingjia,xing,jianjie,url) values(%s,%s,%s,%s,%s)&quot;,params)</div><div class="line">        self.conn.commit()</div><div class="line"></div><div class="line">        return item</div><div class="line"></div><div class="line">    def close_spider(self, spider):</div><div class="line">        self.cs.close()</div><div class="line">        self.conn.close()</div></pre></td></tr></table></figure>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">其他的文件都用mongodb的那些。</div><div class="line">没做去重处理，也没加异常。</div><div class="line">建表的时候有点弱智了。。</div><div class="line">没加id字段。。。</div></pre></td></tr></table></figure>
<p><img src="http://owrmua5nw.bkt.clouddn.com/mysql1.png" alt="mysql1"></p>
<h1 id="scrapy-redis分布式"><a href="#scrapy-redis分布式" class="headerlink" title="scrapy-redis分布式"></a>scrapy-redis分布式</h1><p>未完待续。</p>
<p>个人理解，仅供参考。</p>

      
    </div>
    
    
    

    

    

    
      <div>
        <ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者：</strong>
    lyxhh
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="http://www.lxhsec.com/2017/11/28/scrapy/" title="Scrapy">http://www.lxhsec.com/2017/11/28/scrapy/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>
    本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" rel="external nofollow" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！
  </li>
</ul>

      </div>
    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Python/" rel="tag"><i class="fa fa-tag"></i>Python</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/11/27/redis/" rel="next" title="Redis">
                <i class="fa fa-chevron-left"></i> Redis
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/12/01/xss/" rel="prev" title="XSS">
                XSS <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/1.jpg"
                alt="lyxhh" />
            
              <p class="site-author-name" itemprop="name">lyxhh</p>
              <p class="site-description motion-element" itemprop="description">有你给的阳光，没有理由绝望。</p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/">
              
                  <span class="site-state-item-count">23</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">10</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">15</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          <div class="links-of-author motion-element">
            
          </div>

          
          

          
          
            <div class="links-of-blogroll motion-element links-of-blogroll-block">
              <div class="links-of-blogroll-title">
                <i class="fa  fa-fw fa-link"></i>
                Links
              </div>
              <ul class="links-of-blogroll-list">
                
                  <li class="links-of-blogroll-item">
                    <a href="http://www.secbug.org/" title="破晓团队" target="_blank">破晓团队</a>
                  </li>
                
              </ul>
            </div>
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Scrapy"><span class="nav-number">1.</span> <span class="nav-text">Scrapy</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#简介"><span class="nav-number">1.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基本结构"><span class="nav-number">1.2.</span> <span class="nav-text">基本结构</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#基本流程"><span class="nav-number">1.3.</span> <span class="nav-text">基本流程</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#入门教程"><span class="nav-number">1.4.</span> <span class="nav-text">入门教程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#创建项目"><span class="nav-number">1.4.1.</span> <span class="nav-text">创建项目</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#定义item"><span class="nav-number">1.4.2.</span> <span class="nav-text">定义item</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#编写第一个爬虫"><span class="nav-number">1.4.3.</span> <span class="nav-text">编写第一个爬虫</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分析"><span class="nav-number">1.4.4.</span> <span class="nav-text">分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#爬取"><span class="nav-number">1.4.5.</span> <span class="nav-text">爬取</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#extract"><span class="nav-number">1.4.6.</span> <span class="nav-text">extract()</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#CrawlSpiders"><span class="nav-number">1.5.</span> <span class="nav-text">CrawlSpiders</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#settings-py"><span class="nav-number">1.5.1.</span> <span class="nav-text">settings.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#middlewares-py"><span class="nav-number">1.5.2.</span> <span class="nav-text">middlewares.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pipelines-py"><span class="nav-number">1.5.3.</span> <span class="nav-text">pipelines.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#freebufcrawl-py"><span class="nav-number">1.5.4.</span> <span class="nav-text">freebufcrawl.py</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#item-py"><span class="nav-number">1.5.5.</span> <span class="nav-text">item.py</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#模拟POST"><span class="nav-number">1.6.</span> <span class="nav-text">模拟POST</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#POST请求"><span class="nav-number">1.6.1.</span> <span class="nav-text">POST请求</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#模拟用户登录"><span class="nav-number">1.6.2.</span> <span class="nav-text">模拟用户登录</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cookie"><span class="nav-number">1.6.3.</span> <span class="nav-text">Cookie</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#导入数据库"><span class="nav-number">1.7.</span> <span class="nav-text">导入数据库</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Mongodb"><span class="nav-number">1.7.1.</span> <span class="nav-text">Mongodb</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Mysql"><span class="nav-number">1.7.2.</span> <span class="nav-text">Mysql</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#scrapy-redis分布式"><span class="nav-number">2.</span> <span class="nav-text">scrapy-redis分布式</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">lyxhh</span>

  
</div>





        
<div class="busuanzi-count">
  <script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

  
    <span class="site-uv">
      <i class="fa fa-user"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
      
    </span>
  

  
    <span class="site-pv">
      <i class="fa fa-eye"></i>
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
      
    </span>
  
</div>








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.3"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.3"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.3"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.3"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.3"></script>



  


  




	





  





  












  





  

  

  
  

  

  

  

</body>
</html>
